const axios = require("axios");
const cheerio = require("cheerio");
const XMLWriter = require("xml-writer");
const { URL } = require("url");

const externalUrlRegEx = new RegExp("^(?:[a-z]+:)?//", "i");

/**
 * Returns true if the url is an external url.
 * Its is deemed external if its absolute and the domain differs from the base domain.
 * @param {string} url  url to test
 * @param {*} base the base url of the site we are crawling
 */
function isExternalUrl(url, base) {
  let result = externalUrlRegEx.test(url);
  if (result) {
    // If the url is abs we need to check the host to see if its really a different domain
    const testUrl = new URL(url);
    const baseUrl = new URL(base);
    result = testUrl.host !== baseUrl.host;
  }
  return result;
}

/**
 * Convert a relative url to an absolute one.
 * Supports 'page.html' and '/page.html'
 * @param {string} url
 * @param {string} base
 */
function relToAbsUrl(url, base) {
  const absUrl = new URL(url, base);
  return absUrl.toString();
}

/**
 * Returns true if we can crawl this url.
 *
 * @param {string} url url to test
 */
function isCrawlableUrl(url) {
  // TODO: Exclude other protocals and restricted urls
  return !url.startsWith("mailto:");
}

/**
 * Remove parts of that are known not to change the page content e.g. hash
 * @param {string} url to clean
 */
function cleanUrl(url) {
  //TODO: Remove  utm url params
  const u = new URL(url);
  u.hash = "";
  return u.toString();
}

/**
 * Generate a simple xml output for the crawled site links
 * @param {string} base base url used for site crawl
 * @param {object} siteLinks sitelinks object generated by the crawl
 */
function siteLinksToXml(base, siteLinks) {
  const xml = new XMLWriter();
  xml.startDocument();
  xml.startElement("root");
  xml.writeAttribute("base", base);
  Object.keys(siteLinks).forEach(link => {
    xml.startElement("url");
    xml.text(link);
    xml.endElement();
  });
  return xml.toString();
}

/**
 * Extracts all links from the page and partitions them in to internal, external and resource lists
 * @param {string} html html of page to be processed 
 * @param {string} base base url for the crawl
 */
function processPage(html, base) {
  const result = {
    internal: [],
    external: [],
    resources: []
  }
  const $ = cheerio.load(html);
  /*
      Partition internal, external and image links to separate lists
    */
  $("a").each(function(i, a) {
    const link = $(a).attr("href");
    if (isExternalUrl(link, base)) {
      result.external.push(link);
    } else {
      result.internal.push(relToAbsUrl(link, base));
    }
  });
  $("img").each(function(i, img) {
    const link = $(img).attr("src");
    if (isExternalUrl(link, base)) {
      result.resources.push(link);
    } else {
      result.resources.push(relToAbsUrl(link, base));
    }
  });
  return result;
}

/**
 * @typedef {Object} PageResults
 * @property {string} base The base url specified by the caller
 * @property {string} pageUrl Full url of the page processed
 * @property {string} responseUrl Url of the page that we got the response from e.g. redirects
 * @property {array} external Array of links to external pages
 * @property {array} internal Array of links to internal pages in absolute format
 * @property {array} resources Array of links to any resource referenced by the page e.g. images
 */
/**
 * Downloads and processes the specified page and returns the pages hyperlinks
 * partitioned in to types.
 * @param {string} base host url e.g. https://www.barvas.com
 * @param {string} pageUrl full url of the page to crawl
 * @return {PageResults}
 */
async function getPageLinks(base, pageUrl) {
  const res = await axios.get(pageUrl);
  // If we were redirected the responseUrl will be the final url.
  // return it so it can be marked as visited
  const pageLinks = processPage(res.data, base);
  return {
    base,
    pageUrl,
    responseUrl: res.request.res.responseUrl,
    ...pageLinks
  };
}

module.exports = {
  isExternalUrl,
  cleanUrl,
  isCrawlableUrl,
  getPageLinks,
  processPage,
  siteLinksToXml
};
